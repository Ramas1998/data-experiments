{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thematic Clustering of Fact-Checked Stories\n",
    "\n",
    "This notebook clusters the headlines of fact-checking stories in the Tattle archive using an algorithm called GSDMM. The output file is used to generate a visualisation on the Tattle website.\n",
    "\n",
    "### Process\n",
    "\n",
    "\n",
    "1. Getting the data from MongoDB \n",
    "2. Text cleaning (removing noise, English / non-English headlines separation using regex)\n",
    "3. Translating non-English headlines \n",
    "4. Pre-processing all the headlines (tokenizing, stop word removal, lemmatizing, creating bigrams)\n",
    "5. Text transformation: creating a corpus of vectors\n",
    "6. Building the GSDMM model\n",
    "7. Adding cluster labels to headlines\n",
    "8. Interactive model visualisation with pyLDAvis\n",
    "9. Adding article links and total count to output file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from time import sleep\n",
    "from random import uniform\n",
    "import datetime\n",
    "from datetime import date, timezone\n",
    "import csv\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from os import environ\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models import CoherenceModel \n",
    "import re\n",
    "from langdetect import detect\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import RegexpTokenizer as regextoken\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "#logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from gensim import similarities\n",
    "import nbconvert\n",
    "from gsdmm import MovieGroupProcess\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from factchecking sites MongoDB\n",
    "def initialize_mongo():\n",
    "    mongo_url = \"mongodb+srv://\"+os.environ.get(\"FACTCHECK_DB_USERNAME\")+\":\"+os.environ.get(\"FACTCHECK_DB_PASSWORD\")+\"@tattle-data-fkpmg.mongodb.net/test?retryWrites=true&w=majority&ssl=true&ssl_cert_reqs=CERT_NONE\"   \n",
    "    cli = MongoClient(mongo_url)\n",
    "    db = cli[os.environ.get(\"FACTCHECK_DB_NAME\")]\n",
    "    coll = db[os.environ.get(\"FACTCHECK_DB_COLLECTION\")]\n",
    "    if coll.count_documents({}) > 0:\n",
    "        return coll \n",
    "    else:\n",
    "        print(\"Error accessing Mongo collection\")\n",
    "        sys.exit()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = initialize_mongo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16361"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weekly_data(coll):\n",
    "    pipeline = [\n",
    "        {\"$project\":{\"date_accessed\":\"$date_accessed\", \"date_updated\":\"$date_updated\", 'postID': \"$postID\",'postURL': \"$postURL\",\n",
    "                     \"headline\": \"$headline\", \"docs\": \"$docs\", \"author\": \"$author\", \"domain\": \"$domain\",\n",
    "                     \"date\": {\"$dateFromString\": {\"dateString\": \"$date_accessed\"}}}},\n",
    "        {\"$match\": {\"date\": {\"$gte\":datetime.datetime(2020, 8, 31, 0, 0), \"$lt\": datetime.datetime(2020, 9, 7, 0, 0)}}}\n",
    "    ]\n",
    "    \n",
    "    docs = coll.aggregate(pipeline)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "result=[]\n",
    "docs = get_weekly_data(coll)\n",
    "for doc in docs:\n",
    "    result.append(doc)\n",
    "    c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>date_accessed</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>docs</th>\n",
       "      <th>domain</th>\n",
       "      <th>headline</th>\n",
       "      <th>postID</th>\n",
       "      <th>postURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5f4c9e15efd3af927ab39ab7</td>\n",
       "      <td>{'name': 'Archit Mehta', 'link': 'https://www....</td>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>August 31, 2020</td>\n",
       "      <td>August 25, 2020</td>\n",
       "      <td>[{'doc_id': '948bafd1d6804121ba58f314c61d03c2'...</td>\n",
       "      <td>altnews.in</td>\n",
       "      <td>Video of personal dispute in Hyderabad shared ...</td>\n",
       "      <td>6a7a0b18a49c4ebf9c12a6d89aed5629</td>\n",
       "      <td>https://www.altnews.in/video-of-personal-dispu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5f4c9e1aefd3af927ab39ab8</td>\n",
       "      <td>{'name': 'Kinjal', 'link': 'https://www.altnew...</td>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>August 31, 2020</td>\n",
       "      <td>August 25, 2020</td>\n",
       "      <td>[{'doc_id': '8b249cbbcdb440de915ba60a4939a0e1'...</td>\n",
       "      <td>altnews.in</td>\n",
       "      <td>No, this is not Facebook’s Ankhi Das cutting t...</td>\n",
       "      <td>b0fc3d997ce643d69a7720ea23767931</td>\n",
       "      <td>https://www.altnews.in/fact-check-image-of-amb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5f4c9e1eefd3af927ab39ab9</td>\n",
       "      <td>{'name': 'Priyanka Jha', 'link': 'https://www....</td>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>August 31, 2020</td>\n",
       "      <td>August 26, 2020</td>\n",
       "      <td>[{'doc_id': '9dc9737543e6426aa1f42c07787535ef'...</td>\n",
       "      <td>altnews.in</td>\n",
       "      <td>Video from West Bengal passed off as communal ...</td>\n",
       "      <td>e556d8fe9192452d858965581230e7eb</td>\n",
       "      <td>https://www.altnews.in/video-of-angry-mob-from...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  5f4c9e15efd3af927ab39ab7   \n",
       "1  5f4c9e1aefd3af927ab39ab8   \n",
       "2  5f4c9e1eefd3af927ab39ab9   \n",
       "\n",
       "                                              author       date  \\\n",
       "0  {'name': 'Archit Mehta', 'link': 'https://www.... 2020-08-31   \n",
       "1  {'name': 'Kinjal', 'link': 'https://www.altnew... 2020-08-31   \n",
       "2  {'name': 'Priyanka Jha', 'link': 'https://www.... 2020-08-31   \n",
       "\n",
       "     date_accessed     date_updated  \\\n",
       "0  August 31, 2020  August 25, 2020   \n",
       "1  August 31, 2020  August 25, 2020   \n",
       "2  August 31, 2020  August 26, 2020   \n",
       "\n",
       "                                                docs      domain  \\\n",
       "0  [{'doc_id': '948bafd1d6804121ba58f314c61d03c2'...  altnews.in   \n",
       "1  [{'doc_id': '8b249cbbcdb440de915ba60a4939a0e1'...  altnews.in   \n",
       "2  [{'doc_id': '9dc9737543e6426aa1f42c07787535ef'...  altnews.in   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Video of personal dispute in Hyderabad shared ...   \n",
       "1  No, this is not Facebook’s Ankhi Das cutting t...   \n",
       "2  Video from West Bengal passed off as communal ...   \n",
       "\n",
       "                             postID  \\\n",
       "0  6a7a0b18a49c4ebf9c12a6d89aed5629   \n",
       "1  b0fc3d997ce643d69a7720ea23767931   \n",
       "2  e556d8fe9192452d858965581230e7eb   \n",
       "\n",
       "                                             postURL  \n",
       "0  https://www.altnews.in/video-of-personal-dispu...  \n",
       "1  https://www.altnews.in/fact-check-image-of-amb...  \n",
       "2  https://www.altnews.in/video-of-angry-mob-from...  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_df = df.drop_duplicates(subset=[\"postURL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Video of personal dispute in Hyderabad shared ...\n",
       "1    No, this is not Facebook’s Ankhi Das cutting t...\n",
       "2    Video from West Bengal passed off as communal ...\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Snapshot of headlines\n",
    "clean_df[\"headline\"][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save headlines in a variable\n",
    "raw_data = clean_df[\"headline\"].values.tolist()\n",
    "\n",
    "# Defining a function to remove accented characters in the headlines  \n",
    "def data_dict(sentences):\n",
    "    return dict((sentence, \", \".join(simple_preprocess(str(sentence), deacc=True, max_len=100))) for sentence in sentences)\n",
    "\n",
    "result = data_dict(raw_data)\n",
    "\n",
    "# Separating non-English headlines using regex\n",
    "pat = re.compile(\"[^\\x00-\\x7F]\") # matches non-English characters\n",
    "non_eng = [k for k,v in result.items() if pat.search(v)]\n",
    "eng = [k for k,v in result.items() if not pat.search(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translating non-English headlines\n",
    "\n",
    "Googletrans is a free library that sends translation requests to the Google Translate API. \n",
    "Random time delays between requests are advised, else Google may (and probably will) block your ip address.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating non-English headlines using googletrans library\n",
    "\n",
    "translator = Translator()\n",
    "translations = []\n",
    "for doc in non_eng:\n",
    "    translations.append(translator.translate(doc).text)\n",
    "    time.sleep(uniform(3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the original and translated headlines for future reference\n",
    "translated_headlines = dict(zip(non_eng, translations))\n",
    "translations_df = pd.DataFrame(translated_headlines.items(), columns = [\"headline\", \"translation\"])\n",
    "translations_df[\"original_english\"] = 0\n",
    "translations_df = translations_df.append(pd.DataFrame(eng, columns=['headline']), ignore_index=True, sort=True)\n",
    "translations_df[\"original_english\"].fillna(value=1, inplace = True)\n",
    "translations_df[\"original_english\"] = translations_df[\"original_english\"].astype(int)\n",
    "translations_df.to_csv(\"headlines_with_translations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translations_df = pd.read_csv(\"working-files/headlines_with_translations.csv\")\n",
    "# translations_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translations = list(translations_df[translations_df[\"original_english\"] == 0][\"translation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng+translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the headlines\n",
    "all_headlines = eng + translations\n",
    "# Tokenizing the headlines\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield (simple_preprocess(str(sentence), deacc = True))\n",
    "        \n",
    "all_tokens = list(sent_to_words(all_headlines))\n",
    "# Creating stop words list\n",
    "stop_words = stopwords.words(\"english\")\n",
    "# Adding domain words \n",
    "stop_words.extend([\"fake\", \"fact\", \"check\", \"checked\", \"factcheck\", \"news\", \"false\", \n",
    "                   \"falsely\", \"true\", \"truth\", \"viral\", \"video\", \"image\", \"picture\", \n",
    "                   \"photo\", \"claim\", \"claiming\", \"share\", \"clip\", \"misleading\",\"recent\", \"old\",\n",
    "                  \"india\", \"post\", \"medium\"])\n",
    "# Stop word removal\n",
    "data_stopped = [[word for word in doc if word not in stop_words] for doc in all_tokens]\n",
    "# Creating bigrams\n",
    "bigram = gensim.models.Phrases(data_stopped, min_count=2)\n",
    "for idx in range(len(data_stopped)):\n",
    "    for token in bigram[data_stopped[idx]]:\n",
    "        if '_' in token:\n",
    "            # If token is bigram, add it to document\n",
    "            data_stopped[idx].append(token)\n",
    "            \n",
    "data_with_bigrams = data_stopped\n",
    "# Lemmatizing i.e. reducing words to their root form\n",
    "# Including only nouns as this improves both topic interpretability and coherence scores\n",
    "def lemmatization(docs, allowed_postags=[\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    docs_out = []\n",
    "    for sent in docs:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        docs_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags]) \n",
    "    return docs_out\n",
    "\n",
    "data_lemmatized = lemmatization(data_with_bigrams, allowed_postags=[\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"])\n",
    "\n",
    "# Removing any stopwords created because of lemmatization\n",
    "data_cleaned = [[word for word in doc if word not in stop_words] for doc in data_lemmatized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text transformation: creating a corpus\n",
    "\n",
    "Topic modelling with the Gensim library involves documents, corpus, vectors and bag of words. These are explained here - https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 585\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary\n",
    "id2word = corpora.Dictionary(data_cleaned)\n",
    "# Creating a document-term matrix\n",
    "print('Number of unique tokens: %d' % len(id2word))\n",
    "#id2word.filter_extremes(no_below = 20)\n",
    "# Creating a document-term matrix\n",
    "corpus = [id2word.doc2bow(doc) for doc in data_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['personal', 'dispute', 'hyderabad', 'communal', 'angle', 'shared_communal']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSDMM clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgp = MovieGroupProcess(K=5, alpha=0.1, beta=0.05, n_iters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 92 clusters with 5 clusters populated\n",
      "In stage 1: transferred 36 clusters with 5 clusters populated\n",
      "In stage 2: transferred 36 clusters with 5 clusters populated\n",
      "In stage 3: transferred 29 clusters with 5 clusters populated\n",
      "In stage 4: transferred 29 clusters with 5 clusters populated\n",
      "In stage 5: transferred 25 clusters with 5 clusters populated\n",
      "In stage 6: transferred 23 clusters with 5 clusters populated\n",
      "In stage 7: transferred 19 clusters with 5 clusters populated\n",
      "In stage 8: transferred 18 clusters with 5 clusters populated\n",
      "In stage 9: transferred 20 clusters with 5 clusters populated\n",
      "In stage 10: transferred 14 clusters with 5 clusters populated\n",
      "In stage 11: transferred 12 clusters with 5 clusters populated\n",
      "In stage 12: transferred 17 clusters with 5 clusters populated\n",
      "In stage 13: transferred 25 clusters with 5 clusters populated\n",
      "In stage 14: transferred 18 clusters with 5 clusters populated\n",
      "In stage 15: transferred 20 clusters with 5 clusters populated\n",
      "In stage 16: transferred 22 clusters with 5 clusters populated\n",
      "In stage 17: transferred 12 clusters with 5 clusters populated\n",
      "In stage 18: transferred 22 clusters with 5 clusters populated\n",
      "In stage 19: transferred 18 clusters with 5 clusters populated\n",
      "In stage 20: transferred 15 clusters with 5 clusters populated\n",
      "In stage 21: transferred 16 clusters with 5 clusters populated\n",
      "In stage 22: transferred 22 clusters with 5 clusters populated\n",
      "In stage 23: transferred 21 clusters with 5 clusters populated\n",
      "In stage 24: transferred 20 clusters with 5 clusters populated\n",
      "In stage 25: transferred 15 clusters with 5 clusters populated\n",
      "In stage 26: transferred 18 clusters with 5 clusters populated\n",
      "In stage 27: transferred 20 clusters with 5 clusters populated\n",
      "In stage 28: transferred 21 clusters with 5 clusters populated\n",
      "In stage 29: transferred 16 clusters with 5 clusters populated\n"
     ]
    }
   ],
   "source": [
    "vocab = set(x for doc in data_cleaned for x in doc)\n",
    "y = mgp.fit(data_cleaned,len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cluster_importance(mgp):\n",
    "#     n_z_w = mgp.cluster_word_distribution\n",
    "#     beta, V, K = mgp.beta, mgp.vocab_size, mgp.K\n",
    "#     phi = [{} for i in range(K)]        \n",
    "#     for z in range(K):\n",
    "#         for w in n_z_w[z]:\n",
    "#             phi[z][w] = (n_z_w[z][w]+beta)/(sum(n_z_w[z].values())+V*beta)\n",
    "#     return phi\n",
    "# phi = cluster_importance(mgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic : [21 15 27 20 28]\n",
      "Most important clusters (by number of docs inside): [4 2 0 3 1]\n"
     ]
    }
   ],
   "source": [
    "# doc_count = np.array(mgp.cluster_doc_count)\n",
    "# print('Number of documents per topic :', doc_count)\n",
    "\n",
    "# # Topics sorted by the number of documents they are allocated to\n",
    "# top_index = doc_count.argsort()[-50:][::-1]\n",
    "# print('Most important clusters (by number of docs inside):', top_index)\n",
    "\n",
    "# def top_words(cluster_word_distribution, top_cluster, values):\n",
    "#     for cluster in top_cluster:\n",
    "#         sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "#         print(\"Cluster {} : {}\".format(cluster,sort_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def top_words(cluster_word_distribution, top_cluster, values):\n",
    "#     freq_dict = {}\n",
    "#     for cluster in top_cluster:\n",
    "#         sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "#         freq_dict[str(cluster)] = sort_dicts\n",
    "# #        print(\"Cluster {} : {}\".format(cluster,sort_dicts))\n",
    "#     return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4': [('khan', 7),\n",
       "  ('painting', 4),\n",
       "  ('krishna', 4),\n",
       "  ('mumbai', 4),\n",
       "  ('hyderabad', 4)],\n",
       " '2': [('show', 6), ('new', 5), ('modi', 4), ('pm_modi', 4), ('firework', 4)],\n",
       " '0': [('road', 4), ('delhi', 4), ('bjp', 4), ('link', 3), ('fall', 3)],\n",
       " '3': [('rss', 5), ('man', 4), ('flag', 4), ('indian', 4), ('garden', 4)],\n",
       " '1': [('flag', 9),\n",
       "  ('national', 5),\n",
       "  ('day', 4),\n",
       "  ('national_flag', 3),\n",
       "  ('light', 3)]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top_words(mgp.cluster_word_distribution, top_index, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_counts = top_words(mgp.cluster_word_distribution, top_index, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open (\"wordcounts_wk35.json\", \"w\") as fp:\n",
    "#     json.dump(word_counts, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[\"tokens\"]=data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign cluster label to each article headline\n",
    "clean_df[\"cluster\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>cluster</th>\n",
       "      <th>postURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Video of personal dispute in Hyderabad shared ...</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.altnews.in/video-of-personal-dispu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No, this is not Facebook’s Ankhi Das cutting t...</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.altnews.in/fact-check-image-of-amb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Video from West Bengal passed off as communal ...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.altnews.in/video-of-angry-mob-from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Video of Delhi cop beating minor is not an old...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.altnews.in/video-of-delhi-cop-beat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Congress shares 2012 image of PM Modi with duc...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.altnews.in/congress-shares-2012-im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  cluster  \\\n",
       "0  Video of personal dispute in Hyderabad shared ...        3   \n",
       "1  No, this is not Facebook’s Ankhi Das cutting t...        4   \n",
       "2  Video from West Bengal passed off as communal ...        0   \n",
       "3  Video of Delhi cop beating minor is not an old...        1   \n",
       "4  Congress shares 2012 image of PM Modi with duc...        0   \n",
       "\n",
       "                                             postURL  \n",
       "0  https://www.altnews.in/video-of-personal-dispu...  \n",
       "1  https://www.altnews.in/fact-check-image-of-amb...  \n",
       "2  https://www.altnews.in/video-of-angry-mob-from...  \n",
       "3  https://www.altnews.in/video-of-delhi-cop-beat...  \n",
       "4  https://www.altnews.in/congress-shares-2012-im...  "
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head(5)[[\"headline\", \"cluster\", \"postURL\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    39\n",
       "2    33\n",
       "1    30\n",
       "3    29\n",
       "4    23\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df[\"cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# with open(\"wk35_mgp.model\", \"wb\") as f:\n",
    "#     pickle.dump(mgp, f)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'author', 'date', 'date_accessed', 'date_updated', 'docs',\n",
       "       'domain', 'headline', 'postID', 'postURL', 'tokens', 'cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of clusters and headlines\n",
    "headlines_dict= {}\n",
    "c=1\n",
    "for i in clean_df.groupby(by=\"cluster\"):\n",
    "    pairs_list = []\n",
    "    df = i[1][[\"headline\", \"postURL\", \"tokens\"]]\n",
    "#     print(df[\"tokens\"])\n",
    "#     print(\"...........\")\n",
    "    for idx, row in df.iterrows():\n",
    "        pairs = {}\n",
    "        pairs[\"url\"] = row[\"postURL\"]\n",
    "        pairs[\"headline\"] = row[\"headline\"]\n",
    "        pairs_list.append(pairs)\n",
    "    headlines_dict[c] = pairs_list\n",
    "    c+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.altnews.in/video-of-delhi-cop-beating-minor-is-not-an-old-video-of-armed-chain-snatcher/',\n",
       "  'headline': 'Video of Delhi cop beating minor is not an old video of armed chain snatcher'},\n",
       " {'url': 'https://www.altnews.in/bjp-shares-old-imf-data-to-make-misleading-claim-about-indian-gdp-growth-projection/',\n",
       "  'headline': 'BJP shares old IMF data to make misleading claim about Indian GDP growth projection'}]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines_dict[2][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.altnews.in/video-of-angry-mob-from-west-bengal-shared-as-banglore-with-false-communal-angle/',\n",
       "  'headline': 'Video from West Bengal passed off as communal violence in Bengaluru'},\n",
       " {'url': 'https://www.altnews.in/congress-shares-2012-image-of-pm-modi-with-ducks-as-pr-during-covid-19/',\n",
       "  'headline': 'Congress shares 2012 image of PM Modi with ducks as PR during COVID-19'}]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines_dict[1][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise MGP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(vocab)\n",
    "doc_topic_dists = [mgp.score(doc) for doc in data_cleaned]\n",
    "doc_lengths = [len(doc) for doc in data_cleaned]\n",
    "term_counts_map = {}\n",
    "for doc in data_cleaned:\n",
    "    for term in doc:\n",
    "        term_counts_map[term] = term_counts_map.get(term, 0) + 1\n",
    "term_counts = [term_counts_map[term] for term in vocabulary]\n",
    "\n",
    "matrix = []\n",
    "for cluster in mgp.cluster_word_distribution:\n",
    "    total = sum([occurence for word, occurence in cluster.items()])\n",
    "    row = [cluster.get(term, 0) / total for term in vocabulary]\n",
    "    matrix.append(row)\n",
    "\n",
    "vis_data = pyLDAvis.prepare(topic_term_dists=matrix, doc_topic_dists=doc_topic_dists, doc_lengths=doc_lengths, \n",
    "                            vocab=vocabulary, R=10, term_frequency=term_counts, sort_topics=False)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_json(vis_data, \"wk36.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add headline links and article count to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wk36.json\", \"r\") as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"per_cluster_headlines\"] = headlines_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"number_of_articles\"] = len(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file that will be used in the themes dashboard\n",
    "with open(\"wk36.json\", \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
