{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thematic Clustering of Fact-Checked Stories\n",
    "\n",
    "This notebook clusters the headlines of fact-checking stories in the Tattle archive using an algorithm called GSDMM. The output file is used to generate a visualisation on the Tattle website.\n",
    "\n",
    "### Process\n",
    "\n",
    "\n",
    "1. Getting the data from MongoDB \n",
    "2. Text cleaning (removing noise, English / non-English headlines separation using regex)\n",
    "3. Translating non-English headlines \n",
    "4. Pre-processing all the headlines (tokenizing, stop word removal, lemmatizing, creating bigrams)\n",
    "5. Text transformation: creating a corpus of vectors\n",
    "6. Building the GSDMM model\n",
    "7. Adding cluster labels to headlines\n",
    "8. Interactive model visualisation with pyLDAvis\n",
    "9. Adding article links and total count to output file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from time import sleep\n",
    "from random import uniform\n",
    "import datetime\n",
    "from datetime import date, timezone\n",
    "import csv\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from os import environ\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models import CoherenceModel \n",
    "import re\n",
    "from langdetect import detect\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import RegexpTokenizer as regextoken\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "#logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from gensim import similarities\n",
    "import nbconvert\n",
    "from gsdmm import MovieGroupProcess\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from factchecking sites MongoDB\n",
    "def initialize_mongo():\n",
    "    mongo_url = \"mongodb+srv://\"+os.environ.get(\"FACTCHECK_DB_USERNAME\")+\":\"+os.environ.get(\"FACTCHECK_DB_PASSWORD\")+\"@tattle-data-fkpmg.mongodb.net/test?retryWrites=true&w=majority&ssl=true&ssl_cert_reqs=CERT_NONE\"   \n",
    "    cli = MongoClient(mongo_url)\n",
    "    db = cli[os.environ.get(\"FACTCHECK_DB_NAME\")]\n",
    "    coll = db[os.environ.get(\"FACTCHECK_DB_COLLECTION\")]\n",
    "    if coll.count_documents({}) > 0:\n",
    "        return coll \n",
    "    else:\n",
    "        print(\"Error accessing Mongo collection\")\n",
    "        sys.exit()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = initialize_mongo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16117"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weekly_data(coll):\n",
    "    pipeline = [\n",
    "        {\"$project\":{\"date_accessed\":\"$date_accessed\", \"date_updated\":\"$date_updated\", 'postID': \"$postID\",'postURL': \"$postURL\",\n",
    "                     \"headline\": \"$headline\", \"docs\": \"$docs\", \"author\": \"$author\", \"domain\": \"$domain\",\n",
    "                     \"date\": {\"$dateFromString\": {\"dateString\": \"$date_accessed\"}}}},\n",
    "        {\"$match\": {\"date\": {\"$gte\":datetime.datetime(2020, 8, 24, 0, 0), \"$lt\": datetime.datetime(2020, 8, 31, 0, 0)}}}\n",
    "    ]\n",
    "    \n",
    "    docs = coll.aggregate(pipeline)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "result=[]\n",
    "docs = get_weekly_data(coll)\n",
    "for doc in docs:\n",
    "    result.append(doc)\n",
    "    c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>date_accessed</th>\n",
       "      <th>docs</th>\n",
       "      <th>domain</th>\n",
       "      <th>headline</th>\n",
       "      <th>postID</th>\n",
       "      <th>postURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5f2a37185b354603ecd9d960</td>\n",
       "      <td>{'name': 'Kinjal', 'link': 'https://www.altnew...</td>\n",
       "      <td>2020-08-05</td>\n",
       "      <td>August 05, 2020</td>\n",
       "      <td>[{'doc_id': 'fef2e7b1481740c5aa88cff4352f161c'...</td>\n",
       "      <td>altnews.in</td>\n",
       "      <td>Video of youth forced to drink urine in Rajast...</td>\n",
       "      <td>f8efe97ebad1429ea7df8d1855243d42</td>\n",
       "      <td>https://www.altnews.in/a-video-from-rajasthan-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5f2a371c5b354603ecd9d961</td>\n",
       "      <td>{'name': 'Priyanka Jha', 'link': 'https://www....</td>\n",
       "      <td>2020-08-05</td>\n",
       "      <td>August 05, 2020</td>\n",
       "      <td>[{'doc_id': 'd454064cf1cb4725959408e2553b1767'...</td>\n",
       "      <td>altnews.in</td>\n",
       "      <td>Media outlet Hindustan shares 9-year-old image...</td>\n",
       "      <td>a1268a8179804100a58d3ddccdcd5d7b</td>\n",
       "      <td>https://www.altnews.in/hindustan-newspaper-sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5f2a37215b354603ecd9d962</td>\n",
       "      <td>{'name': 'Archit Mehta', 'link': 'https://www....</td>\n",
       "      <td>2020-08-05</td>\n",
       "      <td>August 05, 2020</td>\n",
       "      <td>[{'doc_id': 'cafea6fd24174b648afe54e36aae0a2a'...</td>\n",
       "      <td>altnews.in</td>\n",
       "      <td>Photo of Hindu deity Ram on New York’s Times S...</td>\n",
       "      <td>ad25cb0038f54a27ac17cf9643fc5ddb</td>\n",
       "      <td>https://www.altnews.in/photo-of-hindu-deity-ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  5f2a37185b354603ecd9d960   \n",
       "1  5f2a371c5b354603ecd9d961   \n",
       "2  5f2a37215b354603ecd9d962   \n",
       "\n",
       "                                              author       date  \\\n",
       "0  {'name': 'Kinjal', 'link': 'https://www.altnew... 2020-08-05   \n",
       "1  {'name': 'Priyanka Jha', 'link': 'https://www.... 2020-08-05   \n",
       "2  {'name': 'Archit Mehta', 'link': 'https://www.... 2020-08-05   \n",
       "\n",
       "     date_accessed                                               docs  \\\n",
       "0  August 05, 2020  [{'doc_id': 'fef2e7b1481740c5aa88cff4352f161c'...   \n",
       "1  August 05, 2020  [{'doc_id': 'd454064cf1cb4725959408e2553b1767'...   \n",
       "2  August 05, 2020  [{'doc_id': 'cafea6fd24174b648afe54e36aae0a2a'...   \n",
       "\n",
       "       domain                                           headline  \\\n",
       "0  altnews.in  Video of youth forced to drink urine in Rajast...   \n",
       "1  altnews.in  Media outlet Hindustan shares 9-year-old image...   \n",
       "2  altnews.in  Photo of Hindu deity Ram on New York’s Times S...   \n",
       "\n",
       "                             postID  \\\n",
       "0  f8efe97ebad1429ea7df8d1855243d42   \n",
       "1  a1268a8179804100a58d3ddccdcd5d7b   \n",
       "2  ad25cb0038f54a27ac17cf9643fc5ddb   \n",
       "\n",
       "                                             postURL  \n",
       "0  https://www.altnews.in/a-video-from-rajasthan-...  \n",
       "1  https://www.altnews.in/hindustan-newspaper-sha...  \n",
       "2  https://www.altnews.in/photo-of-hindu-deity-ra...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_df = df.drop_duplicates(subset=[\"postURL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Morphed: TIME magazine cover featuring Donald ...\n",
       "1    Derogatory painting of Hindu deity Krishna fro...\n",
       "2    Sushant Singh Rajput is not dancing with his ‘...\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Snapshot of headlines\n",
    "clean_df[\"headline\"][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save headlines in a variable\n",
    "raw_data = clean_df[\"headline\"].values.tolist()\n",
    "\n",
    "# Defining a function to remove accented characters in the headlines  \n",
    "def data_dict(sentences):\n",
    "    return dict((sentence, \", \".join(simple_preprocess(str(sentence), deacc=True, max_len=100))) for sentence in sentences)\n",
    "\n",
    "result = data_dict(raw_data)\n",
    "\n",
    "# Separating non-English headlines using regex\n",
    "pat = re.compile(\"[^\\x00-\\x7F]\") # matches non-English characters\n",
    "non_eng = [k for k,v in result.items() if pat.search(v)]\n",
    "eng = [k for k,v in result.items() if not pat.search(v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translating non-English headlines\n",
    "\n",
    "Googletrans is a free library that sends translation requests to the Google Translate API. \n",
    "Random time delays between requests are advised, else Google may (and probably will) block your ip address.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating non-English headlines using googletrans library\n",
    "\n",
    "translator = Translator()\n",
    "translations = []\n",
    "for doc in non_eng:\n",
    "    translations.append(translator.translate(doc).text)\n",
    "    time.sleep(uniform(3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the original and translated headlines for future reference\n",
    "translated_headlines = dict(zip(non_eng, translations))\n",
    "translations_df = pd.DataFrame(translated_headlines.items(), columns = [\"headline\", \"translation\"])\n",
    "translations_df[\"original_english\"] = 0\n",
    "translations_df = translations_df.append(pd.DataFrame(eng, columns=['headline']), ignore_index=True, sort=True)\n",
    "translations_df[\"original_english\"].fillna(value=1, inplace = True)\n",
    "translations_df[\"original_english\"] = translations_df[\"original_english\"].astype(int)\n",
    "translations_df.to_csv(\"headlines_with_translations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>headline</th>\n",
       "      <th>original_english</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>SDPI और मुस्लिम समुदाय पर शंकराचार्य की मूर्ति...</td>\n",
       "      <td>0</td>\n",
       "      <td>SDPI and wrong alleged to flag up the seer sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>फ़ैक्ट-चेक: बुर्का पहनकर पाकिस्तानी झंडा लहराते...</td>\n",
       "      <td>0</td>\n",
       "      <td>Fact-Czech: the burqa wearing Pakistani flag w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>वीडियो में सुशांत के साथ उनकी कोरियोग्राफ़र डां...</td>\n",
       "      <td>0</td>\n",
       "      <td>The video was their choreographer dance with S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           headline  \\\n",
       "0           0  SDPI और मुस्लिम समुदाय पर शंकराचार्य की मूर्ति...   \n",
       "1           1  फ़ैक्ट-चेक: बुर्का पहनकर पाकिस्तानी झंडा लहराते...   \n",
       "2           2  वीडियो में सुशांत के साथ उनकी कोरियोग्राफ़र डां...   \n",
       "\n",
       "   original_english                                        translation  \n",
       "0                 0  SDPI and wrong alleged to flag up the seer sta...  \n",
       "1                 0  Fact-Czech: the burqa wearing Pakistani flag w...  \n",
       "2                 0  The video was their choreographer dance with S...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans = pd.read_csv(\"headlines_with_translations.csv\")\n",
    "trans.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations_df = trans[trans[\"original_english\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = list(translations_df[\"translation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the headlines\n",
    "all_headlines = eng + translations\n",
    "# Tokenizing the headlines\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield (simple_preprocess(str(sentence), deacc = True))\n",
    "        \n",
    "all_tokens = list(sent_to_words(all_headlines))\n",
    "# Creating stop words list\n",
    "stop_words = stopwords.words(\"english\")\n",
    "# Adding domain words \n",
    "stop_words.extend([\"fake\", \"fact\", \"check\", \"checked\", \"factcheck\", \"news\", \"false\", \n",
    "                   \"falsely\", \"true\", \"truth\", \"viral\", \"video\", \"image\", \"picture\", \n",
    "                   \"photo\", \"claim\", \"claiming\", \"share\", \"clip\", \"misleading\",\"recent\", \"old\",\n",
    "                  \"india\", \"post\", \"medium\"])\n",
    "# Stop word removal\n",
    "data_stopped = [[word for word in doc if word not in stop_words] for doc in all_tokens]\n",
    "# Creating bigrams\n",
    "bigram = gensim.models.Phrases(data_stopped, min_count=2)\n",
    "for idx in range(len(data_stopped)):\n",
    "    for token in bigram[data_stopped[idx]]:\n",
    "        if '_' in token:\n",
    "            # If token is bigram, add it to document\n",
    "            data_stopped[idx].append(token)\n",
    "            \n",
    "data_with_bigrams = data_stopped\n",
    "# Lemmatizing i.e. reducing words to their root form\n",
    "# Including only nouns as this improves both topic interpretability and coherence scores\n",
    "def lemmatization(docs, allowed_postags=[\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    docs_out = []\n",
    "    for sent in docs:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        docs_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags]) \n",
    "    return docs_out\n",
    "\n",
    "data_lemmatized = lemmatization(data_with_bigrams, allowed_postags=[\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"])\n",
    "\n",
    "# Removing any stopwords created because of lemmatization\n",
    "data_cleaned = [[word for word in doc if word not in stop_words] for doc in data_lemmatized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text transformation: creating a corpus\n",
    "\n",
    "Topic modelling with the Gensim library involves documents, corpus, vectors and bag of words. These are explained here - https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 483\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary\n",
    "id2word = corpora.Dictionary(data_cleaned)\n",
    "# Creating a document-term matrix\n",
    "print('Number of unique tokens: %d' % len(id2word))\n",
    "#id2word.filter_extremes(no_below = 20)\n",
    "# Creating a document-term matrix\n",
    "corpus = [id2word.doc2bow(doc) for doc in data_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['morph',\n",
       " 'time',\n",
       " 'magazine',\n",
       " 'cover',\n",
       " 'feature',\n",
       " 'donald',\n",
       " 'trump',\n",
       " 'time',\n",
       " 'go',\n",
       " 'headline']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSDMM clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgp = MovieGroupProcess(K=5, alpha=0.1, beta=0.1, n_iters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 71 clusters with 5 clusters populated\n",
      "In stage 1: transferred 21 clusters with 5 clusters populated\n",
      "In stage 2: transferred 19 clusters with 5 clusters populated\n",
      "In stage 3: transferred 18 clusters with 5 clusters populated\n",
      "In stage 4: transferred 15 clusters with 5 clusters populated\n",
      "In stage 5: transferred 23 clusters with 5 clusters populated\n",
      "In stage 6: transferred 22 clusters with 5 clusters populated\n",
      "In stage 7: transferred 17 clusters with 5 clusters populated\n",
      "In stage 8: transferred 16 clusters with 5 clusters populated\n",
      "In stage 9: transferred 19 clusters with 5 clusters populated\n",
      "In stage 10: transferred 21 clusters with 5 clusters populated\n",
      "In stage 11: transferred 10 clusters with 5 clusters populated\n",
      "In stage 12: transferred 15 clusters with 5 clusters populated\n",
      "In stage 13: transferred 13 clusters with 5 clusters populated\n",
      "In stage 14: transferred 13 clusters with 5 clusters populated\n",
      "In stage 15: transferred 17 clusters with 5 clusters populated\n",
      "In stage 16: transferred 19 clusters with 5 clusters populated\n",
      "In stage 17: transferred 14 clusters with 5 clusters populated\n",
      "In stage 18: transferred 10 clusters with 5 clusters populated\n",
      "In stage 19: transferred 11 clusters with 5 clusters populated\n",
      "In stage 20: transferred 20 clusters with 5 clusters populated\n",
      "In stage 21: transferred 14 clusters with 5 clusters populated\n",
      "In stage 22: transferred 12 clusters with 5 clusters populated\n",
      "In stage 23: transferred 13 clusters with 5 clusters populated\n",
      "In stage 24: transferred 12 clusters with 5 clusters populated\n",
      "In stage 25: transferred 14 clusters with 5 clusters populated\n",
      "In stage 26: transferred 17 clusters with 5 clusters populated\n",
      "In stage 27: transferred 17 clusters with 5 clusters populated\n",
      "In stage 28: transferred 17 clusters with 5 clusters populated\n",
      "In stage 29: transferred 12 clusters with 5 clusters populated\n"
     ]
    }
   ],
   "source": [
    "vocab = set(x for doc in data_cleaned for x in doc)\n",
    "y = mgp.fit(data_cleaned,len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_importance(mgp):\n",
    "    n_z_w = mgp.cluster_word_distribution\n",
    "    beta, V, K = mgp.beta, mgp.vocab_size, mgp.K\n",
    "    phi = [{} for i in range(K)]        \n",
    "    for z in range(K):\n",
    "        for w in n_z_w[z]:\n",
    "            phi[z][w] = (n_z_w[z][w]+beta)/(sum(n_z_w[z].values())+V*beta)\n",
    "    return phi\n",
    "phi = cluster_importance(mgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic : [18 20 22 27 24]\n",
      "Most important clusters (by number of docs inside): [3 4 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "\n",
    "# Topics sorted by the number of documents they are allocated to\n",
    "top_index = doc_count.argsort()[-50:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print(\"Cluster {} : {}\".format(cluster,sort_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    freq_dict = {}\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        freq_dict[str(cluster)] = sort_dicts\n",
    "#        print(\"Cluster {} : {}\".format(cluster,sort_dicts))\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3': [('new', 6), ('covid', 5), ('delhi', 5), ('road', 4), ('link', 4)],\n",
       " '4': [('milk', 4),\n",
       "  ('hyderabad', 4),\n",
       "  ('modi', 4),\n",
       "  ('pm_modi', 4),\n",
       "  ('garden', 4)],\n",
       " '2': [('khan', 6), ('rss', 5), ('man', 4), ('flag', 4), ('singh', 4)],\n",
       " '1': [('flag', 9),\n",
       "  ('trailer', 5),\n",
       "  ('national', 5),\n",
       "  ('day', 4),\n",
       "  ('mahesh', 3)],\n",
       " '0': [('morph', 4),\n",
       "  ('krishna', 4),\n",
       "  ('painting', 4),\n",
       "  ('year', 3),\n",
       "  ('trump', 3)]}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words(mgp.cluster_word_distribution, top_index, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = top_words(mgp.cluster_word_distribution, top_index, 5)\n",
    "type(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"wordcounts_wk35.json\", \"w\") as fp:\n",
    "    json.dump(word_counts, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[\"tokens\"]=data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign cluster label to each article headline\n",
    "clean_df[\"cluster\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>cluster</th>\n",
       "      <th>postURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Morphed: TIME magazine cover featuring Donald ...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.altnews.in/morphed-image-of-time-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Derogatory painting of Hindu deity Krishna fro...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.altnews.in/derogatory-painting-of-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sushant Singh Rajput is not dancing with his ‘...</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.altnews.in/sushant-singh-rajput-da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RSS man dressed in burqa caught by the police ...</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.altnews.in/did-rss-worker-wearing-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Old pornographic visual from Pakistan shared a...</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.altnews.in/old-pornographic-visual...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  cluster  \\\n",
       "0  Morphed: TIME magazine cover featuring Donald ...        1   \n",
       "1  Derogatory painting of Hindu deity Krishna fro...        1   \n",
       "2  Sushant Singh Rajput is not dancing with his ‘...        3   \n",
       "3  RSS man dressed in burqa caught by the police ...        3   \n",
       "4  Old pornographic visual from Pakistan shared a...        3   \n",
       "\n",
       "                                             postURL  \n",
       "0  https://www.altnews.in/morphed-image-of-time-t...  \n",
       "1  https://www.altnews.in/derogatory-painting-of-...  \n",
       "2  https://www.altnews.in/sushant-singh-rajput-da...  \n",
       "3  https://www.altnews.in/did-rss-worker-wearing-...  \n",
       "4  https://www.altnews.in/old-pornographic-visual...  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head(5)[[\"headline\", \"cluster\", \"postURL\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    27\n",
       "5    24\n",
       "3    22\n",
       "2    20\n",
       "1    18\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df[\"cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "with open(\"wk35_mgp.model\", \"wb\") as f:\n",
    "    pickle.dump(mgp, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of clusters and headlines\n",
    "headlines_dict= {}\n",
    "c=1\n",
    "for i in clean_df.groupby(by=\"cluster\"):\n",
    "    #print(i[1][\"postURL\"])\n",
    "    headlines_dict[c] = list(i[1][\"postURL\"])\n",
    "    c+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.altnews.in/morphed-image-of-time-to-go-cover-featuring-donald-trump-shared-by-journalists/',\n",
       " 'https://www.altnews.in/derogatory-painting-of-hindu-deity-krishna-from-5-years-ago-revived-on-social-media/',\n",
       " 'https://www.altnews.in/hindi/sushant-singh-rajput-dancing-with-his-choreographer-manpreet-toor-media-misreports-he-is-dancing-with-his-niece-mallika-singh/',\n",
       " 'https://www.altnews.in/hindi/two-decade-old-hostage-rescue-video-from-venezuela-viral-as-spanish-police-shoot-is-terrorist/',\n",
       " 'https://www.boomlive.in/fake-news/fact-check-did-putins-daughter-die-after-taking-covid-19-vaccine-9437',\n",
       " 'https://hindi.boomlive.in/fake-news/no-mughal-gardens-has-not-been-renamed-to-dr-rajendra-prasad-garden-9406',\n",
       " 'https://bangla.boomlive.in/fake-news/old-video-clip-of-fire-breaking-out-at-delhis-tughlakabad-slum-shared-with-communal-spin-9381',\n",
       " 'https://factly.in/this-man-was-not-beaten-for-hoisting-blue-colour-bsp-flag-instead-of-national-flag/',\n",
       " 'https://factly.in/image-of-pm-modi-bowing-to-gandhis-statue-is-edited-with-nehrus-statue/',\n",
       " 'https://factly.in/this-photo-does-not-show-people-following-bharatiya-culture-in-new-zealand/',\n",
       " 'https://factly.in/unrelated-simulated-fireworks-video-shared-as-the-fireworks-prepared-for-tokyo-2020/',\n",
       " 'https://factly.in/there-is-no-such-scheme-called-pm-kisan-tractor-yojana/',\n",
       " 'https://www.indiatoday.in/fact-check/story/did-burj-khalifa-light-up-with-israeli-flag-after-historic-pact-with-uae-1711885-2020-08-16',\n",
       " 'https://www.indiatoday.in/fact-check/story/fact-check-not-king-bahrain-in-dubai-automaton-bodyguard-1712063-2020-08-17',\n",
       " 'http://newsmobile.in/articles/2020/08/17/fact-check-raga-hoisted-the-flag-on-the-eve-of-74th-independence-day-heres-the-truth/',\n",
       " 'http://newsmobile.in/articles/2020/08/18/fact-check-salman-khan-did-not-call-sushant-singh-rajput-a-coward/',\n",
       " 'http://newsmobile.in/articles/2020/08/18/fact-check-is-kamala-harris-ineligible-to-serve-as-us-president-if-need-be-heres-the-truth/',\n",
       " 'http://newsmobile.in/articles/2020/08/20/fact-check-quote-attributed-to-arbaaz-khan-on-his-brother-salman-khans-movies-is-false/']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines_dict[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise MGP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(vocab)\n",
    "doc_topic_dists = [mgp.score(doc) for doc in data_cleaned]\n",
    "doc_lengths = [len(doc) for doc in data_cleaned]\n",
    "term_counts_map = {}\n",
    "for doc in data_cleaned:\n",
    "    for term in doc:\n",
    "        term_counts_map[term] = term_counts_map.get(term, 0) + 1\n",
    "term_counts = [term_counts_map[term] for term in vocabulary]\n",
    "\n",
    "matrix = []\n",
    "for cluster in mgp.cluster_word_distribution:\n",
    "    total = sum([occurence for word, occurence in cluster.items()])\n",
    "    row = [cluster.get(term, 0) / total for term in vocabulary]\n",
    "    matrix.append(row)\n",
    "\n",
    "vis_data = pyLDAvis.prepare(topic_term_dists=matrix, doc_topic_dists=doc_topic_dists, doc_lengths=doc_lengths, \n",
    "                            vocab=vocabulary, R=10, term_frequency=term_counts, sort_topics=False)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_json(vis_data, \"wk35.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add headline links and article count to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wk35.json\", \"r\") as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"per_cluster_headlines\"] = headlines_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"number_of_articles\"] = len(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file that will be used in the themes dashboard\n",
    "with open(\"wk35.json\", \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
